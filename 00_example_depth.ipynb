{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4cd24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "from typing import Optional\n",
    "from transformers import pipeline, Pipeline\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def open_video(path: str) -> cv2.VideoCapture:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    return cap\n",
    "\n",
    "def next_image_from_video(cap: cv2.VideoCapture) -> Optional[Image.Image]:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "    return image\n",
    "\n",
    "def perform_depth_example(pipe: Pipeline):\n",
    "    url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    # image.show()\n",
    "    predictions = pipe(image)\n",
    "    predictions[\"depth\"].show()\n",
    "\n",
    "def render_first_video_depth(pipe: Pipeline):\n",
    "    plt.ion() # interactive so we can update the image data to \"play the video\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    video = open_video('dataset/studytable_open_drawer/videos/chunk-000/observation.image.camera1_img/episode_000000.mp4')\n",
    "    frame_count = 0\n",
    "\n",
    "    frame = None\n",
    "    first = True\n",
    "    img_display = None\n",
    "    while first or frame is not None:\n",
    "        first = False\n",
    "        frame_count += 1\n",
    "\n",
    "        frame = next_image_from_video(video)\n",
    "\n",
    "        predictions = pipe(frame)\n",
    "        depth = predictions['depth']\n",
    "\n",
    "        if img_display is None:\n",
    "            img_display = ax.imshow(depth) # use frame for original img\n",
    "        else:\n",
    "            img_display.set_data(depth) # use frame for original img\n",
    "            \n",
    "        plt.draw()\n",
    "        plt.title(f'Frame {frame_count}')\n",
    "\n",
    "        plt.pause(0.01)\n",
    "\n",
    "    video.release()\n",
    "    plt.ioff()\n",
    "    plt.close()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    device, _, _ = get_backend()\n",
    "    checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "    pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n",
    "\n",
    "    # perform_depth_example(pipe)\n",
    "    render_first_video_depth(pipe)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
