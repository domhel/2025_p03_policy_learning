{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96316ca2",
   "metadata": {},
   "source": [
    "# Notes from example\n",
    "- telling names for functions\n",
    "- use type hinting for arguments and function returns\n",
    "- 1 feature per function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d2e22",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80112e7c",
   "metadata": {},
   "source": [
    "- [ ] dataset import as recommended by Cagatay\n",
    "- [ ] detailed explanation and annotation in english\n",
    "- [ ] Discuss whether post-processing/formatting of the images, e.g. color map, inversion of the grayscale, further processing of the real depth values should be carried out (apply_model)\n",
    "- [ ] Adapt the postfix in job_agent() or the string value of the prediction_models.value if desired\n",
    "\n",
    "Optionals:\n",
    "- OPTIONAL: If desired, integrate a appropriate return value or any feedback messages\n",
    "- OPTIONAL: Alterantive way to work with a file list instead of directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc058cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Produktiv-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7f217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from transformers import pipeline, Pipeline\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ead18015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Handling\n",
    "#TODO: Data-Import like in Example\n",
    "\n",
    "def open_video(path: str) -> cv2.VideoCapture:\n",
    "    \"\"\"\n",
    "    Opens a video file using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        cv2.VideoCapture: The opened video capture object.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the video file cannot be opened.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    return cap\n",
    "\n",
    "def next_image_from_video(cap: cv2.VideoCapture) -> Optional[Image.Image]:\n",
    "    ret, frame = cap.read()                                 #frame as array with bgr values\n",
    "    if not ret:                                             #if read was not successfull \n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #Image as Array with RGB Values\n",
    "    image = Image.fromarray(frame_rgb)                      #PIL Image\n",
    "    return image\n",
    "\n",
    "def image_to_video(image: Image.Image, video_writer: cv2.VideoWriter):\n",
    "    #convert the video to the required format\n",
    "    image_np_rgb = np.array(image)\n",
    "    depth_bgr = cv2.cvtColor(image_np_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # write frame to output video file\n",
    "    video_writer.write(depth_bgr)\n",
    "\n",
    "class prediction_models(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration of available prediction model types in our pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        GRAYSCALE: converts the video from rgb to grayscale - for for test purposes, a low-computing alternative\n",
    "        DEPTH_ANYTHING_V2: Represents the 'Depth Anything V2' model with Hugging Face ID 'Depth-Anything-V2-base-hf'.\n",
    "    \"\"\"\n",
    "    GRAYSCALE = \"gray\"\n",
    "    DEPTH_ANYTHING_V2 = \"Depth-Anything-V2-base-hf\"\n",
    "\n",
    "class ImageProcessor:\n",
    "    device = None\n",
    "    pipe = None\n",
    "\n",
    "    def apply_model(self, frame: Image.Image ,model_selection: prediction_models) -> Image.Image: \n",
    "        if self.device is None:\n",
    "            device, _, _ = get_backend()\n",
    "            self.device = device\n",
    "\n",
    "        if model_selection == prediction_models.DEPTH_ANYTHING_V2:\n",
    "            checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "            if self.pipe is None:\n",
    "                self.pipe = pipeline(\"depth-estimation\", model=checkpoint, device=self.device)\n",
    "            predictions = self.pipe(frame)\n",
    "            image_w_pred = predictions['depth']\n",
    "\n",
    "        if model_selection == prediction_models.GRAYSCALE:\n",
    "            image_rgb = np.array(frame)\n",
    "            image_w_pred = Image.fromarray(cv2.cvtColor(image_rgb,cv2.COLOR_RGB2GRAY))\n",
    "            \n",
    "\n",
    "        # If all models are called in the same way via the transformers library/pipeline, we can remove the general part and only put the parameterization in the if clause\n",
    "        \n",
    "        return image_w_pred\n",
    "\n",
    "def get_videowriter(cap: cv2.VideoCapture, target_path: str) -> cv2.VideoWriter:\n",
    "    \"\"\"\n",
    "    Creates a cv2.VideoWriter object based on the properties of an existing VideoCapture.\n",
    "\n",
    "    Args:\n",
    "        cap (cv2.VideoCapture): OpenCV video capture object from which to read properties (width, height, fps).\n",
    "        target_path (str): Path to the output video file (e.g., \"output.mp4\").\n",
    "\n",
    "    Returns:\n",
    "        cv2.VideoWriter: OpenCV video writer object configured with H.264 codec.\n",
    "    \"\"\"\n",
    "    # get the video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "\n",
    "    # set the output stream (MP4 mit H.264 - should provide a good video player compatibility) \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'H264')  \n",
    "    video_writer = cv2.VideoWriter(target_path, fourcc, fps, (width, height), isColor=True)\n",
    "    return video_writer\n",
    "\n",
    "def convert_video(src_path: str, target_path: str,selected_model:prediction_models, image_processor: ImageProcessor, test_mode = False):\n",
    "    # run through the video frame by frame, look for errors in the image\n",
    "    cap = open_video(src_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    #initialize the video output \n",
    "    video_writer = get_videowriter(cap,target_path)\n",
    "\n",
    "    #test_mode: if activated only the first 5 seconds will be converted in order to need less computing time\n",
    "    if test_mode==True:\n",
    "        frame_count = int(5*fps)\n",
    "    fps\n",
    "\n",
    "    for i in tqdm(range(frame_count),desc=\"Run through video frame per frame\"):\n",
    "        frame = next_image_from_video(cap)   #PIL Image\n",
    "        if frame == None:\n",
    "            tqdm.write(\"WARNUNG: Kein weiteres Bild gelesen - Video zu Ende oder Fehler beim Zugriff.\")\n",
    "            break\n",
    "        new_image = image_processor.apply_model(frame, model_selection=selected_model)\n",
    "        image_to_video(new_image,video_writer)\n",
    "    \n",
    "    cap.release()\n",
    "    video_writer.release() #After release of video_writer the video will be stored\n",
    "\n",
    "def create_target_path(path_file: str,target_dir: str, selected_model: prediction_models) -> str:\n",
    "    path_valid = True\n",
    "    counter = 0\n",
    "    postfix_raw = selected_model.value\n",
    "    postfix = postfix_raw\n",
    "    while(path_valid):\n",
    "        path_valid = False\n",
    "        basename = Path(path_file).stem\n",
    "        target_name = basename+\"_\"+postfix+\".mp4\"\n",
    "        target_path = Path(target_dir, target_name)\n",
    "        Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "        target_path = str(target_path)\n",
    "        # print(f'{target_path=}')\n",
    "        if os.path.exists(target_path): # only used for handling duplicate files\n",
    "            counter+=1\n",
    "            path_valid = True\n",
    "            postfix = postfix_raw + \"_\"+str(counter)\n",
    "    \n",
    "    return target_path\n",
    "\n",
    "def job_agent(src_directory: str,prediction_models_list: list[prediction_models],target_directory: str,test_mode = False):\n",
    "    #OPTIONAL: If desired, integrate a appropriate return value or any feedback messages\n",
    "    #OPTIONAL: Alterantive way to work with a file list instead of directory\n",
    "    #TODO adapt postfix if desired\n",
    "    #not tested yet\n",
    "    mp4_files = sorted(glob.glob(os.path.join(src_directory,\"*.mp4\")))\n",
    "    image_processor = ImageProcessor()\n",
    "    target_directory = str(Path(target_directory, strftime(\"%Y-%m-%d_%H%M%S\", gmtime())))\n",
    "    for file in mp4_files: # swap after testing\n",
    "        # print(f'{file=}')\n",
    "        for model in prediction_models_list:\n",
    "            target_path = create_target_path(file,target_directory,model)\n",
    "            convert_video(file,target_path,model,test_mode=test_mode, image_processor=image_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b132e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34363248/'H264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n",
      "Run through video frame per frame:   0%|          | 0/50 [00:00<?, ?it/s]Device set to use mps\n",
      "Run through video frame per frame:  24%|██▍       | 12/50 [00:03<00:12,  3.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m prediction_models_list = [prediction_models.DEPTH_ANYTHING_V2]\n\u001b[32m      4\u001b[39m target_directory = \u001b[33m\"\u001b[39m\u001b[33m../output\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mjob_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprediction_models_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mjob_agent\u001b[39m\u001b[34m(src_directory, prediction_models_list, target_directory, test_mode)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m prediction_models_list:\n\u001b[32m    154\u001b[39m     target_path = create_target_path(file,target_directory,model)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[43mconvert_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mconvert_video\u001b[39m\u001b[34m(src_path, target_path, selected_model, image_processor, test_mode)\u001b[39m\n\u001b[32m    115\u001b[39m         tqdm.write(\u001b[33m\"\u001b[39m\u001b[33mWARNUNG: Kein weiteres Bild gelesen - Video zu Ende oder Fehler beim Zugriff.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     new_image = \u001b[43mimage_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     image_to_video(new_image,video_writer)\n\u001b[32m    120\u001b[39m cap.release()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mImageProcessor.apply_model\u001b[39m\u001b[34m(self, frame, model_selection)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mdepth-estimation\u001b[39m\u001b[33m\"\u001b[39m, model=checkpoint, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     image_w_pred = predictions[\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_selection == prediction_models.GRAYSCALE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/depth_estimation.py:93\u001b[39m, in \u001b[36mDepthEstimationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot call the depth-estimation pipeline without an inputs argument!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1287\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m   1286\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._forward(model_inputs, **forward_params)\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.framework\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1187\u001b[39m, in \u001b[36mPipeline._ensure_tensor_on_device\u001b[39m\u001b[34m(self, inputs, device)\u001b[39m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_tensor_on_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, device):\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, ModelOutput):\n\u001b[32m   1186\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m             {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m   1188\u001b[39m         )\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/2025_p03_policy_learning/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1198\u001b[39m, in \u001b[36mPipeline._ensure_tensor_on_device\u001b[39m\u001b[34m(self, inputs, device)\u001b[39m\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch.Tensor):\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#example\n",
    "src_directory = \"../dataset/studytable_open_drawer/videos/chunk-000/observation.image.camera1_img\"\n",
    "prediction_models_list = [prediction_models.DEPTH_ANYTHING_V2]\n",
    "target_directory = \"../output\"\n",
    "job_agent(src_directory,prediction_models_list,target_directory,test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa661af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Test Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695fb87",
   "metadata": {},
   "source": [
    "### Extract single example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5442d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Breite (px)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Höhe (px)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FPS:",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "frames_",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5d9be9ae-c9cc-4177-817e-f3a55c7521b9",
       "rows": [
        [
         "0",
         "320",
         "240",
         "10.0",
         "612"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breite (px)</th>\n",
       "      <th>Höhe (px)</th>\n",
       "      <th>FPS:</th>\n",
       "      <th>frames_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>10.0</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Breite (px)  Höhe (px)  FPS:  frames_\n",
       "0          320        240  10.0      612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap - set\n",
      "cap - read \n"
     ]
    }
   ],
   "source": [
    "#extract single image from video in variable image_example [Image.Image]\n",
    "import pandas as pd\n",
    "\n",
    "path_example = r\"/Users/dominik/uni/2025_p03_policy_learning/dataset/studytable_open_drawer/videos/chunk-000/observation.image.camera1_img/episode_000001.mp4\"\n",
    "cap = cv2.VideoCapture(path_example)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "df_video_data = pd.DataFrame({\n",
    "        'Breite (px)': [width],\n",
    "        'Höhe (px)': [height],\n",
    "        'FPS:':[fps],\n",
    "        'frames_':[frame_count]\n",
    "    })\n",
    "\n",
    "display(df_video_data)\n",
    "\n",
    "\n",
    "frame_number = 315\n",
    "\n",
    "if frame_number<frame_count:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    print(\"cap - set\")\n",
    "    ret, frame = cap.read()\n",
    "    print(\"cap - read \")\n",
    "    if ret:             #read was sucesssful\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #Image as Array with RGB Values\n",
    "        image_example = Image.fromarray(frame_rgb)  \n",
    "        image_example.show()\n",
    "        #image_example.save(r\"C:\\Users\\lehrm\\Downloads\\images\\cam1.jpg\")\n",
    "    else:\n",
    "        print(\"Warning: Frame could not be read!\")\n",
    "\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587af7fe",
   "metadata": {},
   "source": [
    "### Test the model application on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979a68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "new_image_example = apply_model(image_example,model_selection=prediction_models.DEPTH_ANYTHING_V2)\n",
    "new_image_example.show()\n",
    "#new_image_example.save(r\"C:\\Users\\lehrm\\Downloads\\images\\cam1_depth.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e7fee",
   "metadata": {},
   "source": [
    "### Simple run on a video - using the test_mode with editing only 5 sec of the given video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b920cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run through video frame per frame:   0%|          | 0/20 [00:00<?, ?it/s]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▌         | 1/20 [00:03<01:09,  3.64s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|█         | 2/20 [00:06<00:59,  3.32s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▌        | 3/20 [00:10<00:59,  3.52s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|██        | 4/20 [00:14<00:57,  3.57s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▌       | 5/20 [00:18<00:55,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|███       | 6/20 [00:21<00:52,  3.75s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▌      | 7/20 [00:25<00:48,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|████      | 8/20 [00:29<00:44,  3.67s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▌     | 9/20 [00:32<00:39,  3.59s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|█████     | 10/20 [00:36<00:35,  3.56s/it]Device set to use cpu\n",
      "Run through video frame per frame:  55%|█████▌    | 11/20 [00:39<00:31,  3.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  60%|██████    | 12/20 [00:43<00:28,  3.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  65%|██████▌   | 13/20 [00:46<00:24,  3.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  70%|███████   | 14/20 [00:50<00:21,  3.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  75%|███████▌  | 15/20 [00:53<00:17,  3.52s/it]Device set to use cpu\n",
      "Run through video frame per frame:  80%|████████  | 16/20 [00:57<00:14,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  85%|████████▌ | 17/20 [01:02<00:11,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  90%|█████████ | 18/20 [01:06<00:08,  4.16s/it]Device set to use cpu\n",
      "Run through video frame per frame:  95%|█████████▌| 19/20 [01:12<00:04,  4.54s/it]Device set to use cpu\n",
      "Run through video frame per frame: 100%|██████████| 20/20 [01:16<00:00,  3.82s/it]\n"
     ]
    }
   ],
   "source": [
    "path_output = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\output\\grayscale2.mp4\"\n",
    "convert_video(path_example, path_output,prediction_models.DEPTH_ANYTHING_V2,test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db7f07",
   "metadata": {},
   "source": [
    "### Test Datei-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0141e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000000.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000001.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000002.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000003.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000004.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000005.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000006.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000007.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000008.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000009.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000010.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000011.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000012.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000013.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000014.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000015.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000016.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000017.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000018.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000019.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000020.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000021.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000022.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000023.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000024.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000025.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000026.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000027.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000028.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000029.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000030.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000031.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000032.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000033.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000034.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000035.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000036.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000037.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000038.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000039.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000040.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000041.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000042.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000043.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000044.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000045.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000046.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000047.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000048.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000049.mp4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "path_file_dir = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\"\n",
    "\n",
    "mp4_files = glob.glob(os.path.join(path_file_dir,\"*.mp4\"))\n",
    "\n",
    "for datei in mp4_files:\n",
    "    print(datei)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
