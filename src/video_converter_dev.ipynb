{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96316ca2",
   "metadata": {},
   "source": [
    "# Notes from example\n",
    "- telling names for functions\n",
    "- use type hinting for arguments and function returns\n",
    "- 1 feature per function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc058cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Produktiv-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from transformers import pipeline, Pipeline\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ead18015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\lehrm\\AppData\\Local\\Temp\\ipykernel_22620\\2124395856.py:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  target_name = \".\\output\\new_video1.mp4\"\n"
     ]
    }
   ],
   "source": [
    "# Video Handling\n",
    "#TODO: Prüfe Nachbearbeitung Bild erforderlich\n",
    "#TODO: Data-Import like in Example\n",
    "\n",
    "def open_video(path: str) -> cv2.VideoCapture:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "    return cap\n",
    "\n",
    "def next_image_from_video(cap: cv2.VideoCapture) -> Optional[Image.Image]:\n",
    "    ret, frame = cap.read()                                 #frame as array with bgr values\n",
    "    if not ret:                 #read was not successfull \n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #Image as Array with RGB Values\n",
    "    image = Image.fromarray(frame_rgb)                      #PIL Image\n",
    "    return image\n",
    "\n",
    "def image_to_video(image: Image.Image, video_writer: cv2.VideoWriter):\n",
    "    #GGF noch postprocessing des videos, z.B. Normalisieren\n",
    "    image_np_rgb = np.array(image)\n",
    "    depth_bgr = cv2.cvtColor(image_np_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Frame ins Video schreiben\n",
    "    video_writer.write(depth_bgr)\n",
    "\n",
    "class prediction_models(Enum):\n",
    "    GRAYSCALE = \"gray\"\n",
    "    DEPTH_ANYTHING_V2 = \"Depth-Anything-V2-base-hf\"\n",
    "\n",
    "def apply_model(frame: Image.Image ,model_selection: prediction_models) -> Image.Image:\n",
    "    device, _, _ = get_backend()\n",
    "    if model_selection == prediction_models.DEPTH_ANYTHING_V2:\n",
    "        checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "        pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n",
    "        predictions = pipe(frame)\n",
    "        image_w_pred = predictions['depth']\n",
    "\n",
    "    if model_selection == prediction_models.GRAYSCALE:\n",
    "        image_rgb = np.array(frame)\n",
    "        image_w_pred = Image.fromarray(cv2.cvtColor(image_rgb,cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "    # If all models are called in the same way via the transformers library/pipeline, we can remove the general part and only put the parameterization in the if clause\n",
    "    # TODO: set format of image w depth estimation, grayscale, colormar or whatever\n",
    "\n",
    "    return image_w_pred\n",
    "\n",
    "def get_videowriter(cap: cv2.VideoCapture, target_path: str) -> cv2.VideoWriter:\n",
    "    # Video Schreiber erstellen\n",
    "    # Videoeigenschaften holen\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "\n",
    "    # VideoWriter vorbereiten (MP4 mit H.264)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'H264')  # Alternativ: 'avc1', 'XVID', 'H264'\n",
    "    video_writer = cv2.VideoWriter(target_path, fourcc, fps, (width, height), isColor=True)\n",
    "    return video_writer\n",
    "\n",
    "def convert_video(src_path: str, target_path: str,selected_model:prediction_models):\n",
    "    # Video durchlaufen, Fehlermeldungen berücksichtigen\n",
    "    cap = open_video(src_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    #Video Ausgabe initialisieren\n",
    "    video_writer = get_videowriter(cap,target_path)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(frame_count),desc=\"Run through video frame per frame\"):\n",
    "        frame = next_image_from_video(cap)   #PIL Image\n",
    "        if frame == None:\n",
    "            tqdm.write(\"WARNUNG: Kein weiteres Bild gelesen - Video zu Ende oder Fehler beim Zugriff.\")\n",
    "            break\n",
    "        new_image = apply_model(frame, model_selection=selected_model)\n",
    "        image_to_video(new_image,video_writer)\n",
    "    \n",
    "    cap.release()\n",
    "    video_writer.release() #After release of video_writer the video will be stored\n",
    "\n",
    "def job_agent(src_directory: str,prediction_models_list: list):\n",
    "    #TODO: If desired, integrate a appropriate feedback\n",
    "    #TODO: Alterantive way to work with a file list instead of directory\n",
    "    #not tested yet\n",
    "    mp4_files = glob.glob(os.path.join(src_directory,\"*.mp4\"))\n",
    "    for file in mp4_files:\n",
    "        for model in prediction_models_list:\n",
    "            #TODO Rename Function\n",
    "            target_name = \".\\output\\new_video1.mp4\"\n",
    "            convert_video(file,target_name,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b132e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "src_directory = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\"\n",
    "pediction_models_list = [prediction_models.DEPTH_ANYTHING_V2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa661af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Test Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695fb87",
   "metadata": {},
   "source": [
    "### Beispielbild generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Breite (px)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Höhe (px)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FPS:",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "frames_",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b3d12eda-f425-4b0e-86b1-c6e9688793db",
       "rows": [
        [
         "0",
         "320",
         "240",
         "10.0",
         "554"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breite (px)</th>\n",
       "      <th>Höhe (px)</th>\n",
       "      <th>FPS:</th>\n",
       "      <th>frames_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320</td>\n",
       "      <td>240</td>\n",
       "      <td>10.0</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Breite (px)  Höhe (px)  FPS:  frames_\n",
       "0          320        240  10.0      554"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap - set\n",
      "cap - read \n"
     ]
    }
   ],
   "source": [
    "#extract single image from video in variable image_example [Image.Image]\n",
    "import pandas as pd\n",
    "\n",
    "path_example = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000000.mp4\"\n",
    "cap = cv2.VideoCapture(path_example)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "df_video_data = pd.DataFrame({\n",
    "        'Breite (px)': [width],\n",
    "        'Höhe (px)': [height],\n",
    "        'FPS:':[fps],\n",
    "        'frames_':[frame_count]\n",
    "    })\n",
    "\n",
    "display(df_video_data)\n",
    "\n",
    "\n",
    "frame_number = 450\n",
    "\n",
    "if frame_number<frame_count:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    print(\"cap - set\")\n",
    "    ret, frame = cap.read()\n",
    "    print(\"cap - read \")\n",
    "    if ret:             #read was sucesssful\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      #Image as Array with RGB Values\n",
    "        image_example = Image.fromarray(frame_rgb)  \n",
    "        image_example.show()\n",
    "    else:\n",
    "        print(\"Warning: Frame could not be read!\")\n",
    "\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587af7fe",
   "metadata": {},
   "source": [
    "### Bild Umwandlung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "979a68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "new_image_example = apply_model(image_example,model_selection=prediction_models.DEPTH_ANYTHING_V2)\n",
    "new_image_example.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e7fee",
   "metadata": {},
   "source": [
    "### Video Durchlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b920cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run through video frame per frame:   0%|          | 0/554 [00:00<?, ?it/s]Device set to use cpu\n",
      "Run through video frame per frame:   0%|          | 1/554 [00:03<31:16,  3.39s/it]Device set to use cpu\n",
      "Run through video frame per frame:   0%|          | 2/554 [00:06<29:22,  3.19s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|          | 3/554 [00:09<30:33,  3.33s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|          | 4/554 [00:14<34:03,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|          | 5/554 [00:18<34:09,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|          | 6/554 [00:21<33:38,  3.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|▏         | 7/554 [00:25<35:41,  3.91s/it]Device set to use cpu\n",
      "Run through video frame per frame:   1%|▏         | 8/554 [00:29<34:57,  3.84s/it]Device set to use cpu\n",
      "Run through video frame per frame:   2%|▏         | 9/554 [00:33<34:11,  3.76s/it]Device set to use cpu\n",
      "Run through video frame per frame:   2%|▏         | 10/554 [00:36<33:49,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:   2%|▏         | 11/554 [00:40<33:15,  3.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:   2%|▏         | 12/554 [00:44<34:59,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:   2%|▏         | 13/554 [00:48<34:04,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 14/554 [00:52<34:30,  3.83s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 15/554 [00:55<33:35,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 16/554 [00:59<34:13,  3.82s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 17/554 [01:03<34:06,  3.81s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 18/554 [01:07<33:30,  3.75s/it]Device set to use cpu\n",
      "Run through video frame per frame:   3%|▎         | 19/554 [01:10<33:19,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:   4%|▎         | 20/554 [01:17<40:53,  4.59s/it]Device set to use cpu\n",
      "Run through video frame per frame:   4%|▍         | 21/554 [01:22<40:47,  4.59s/it]Device set to use cpu\n",
      "Run through video frame per frame:   4%|▍         | 22/554 [01:26<41:03,  4.63s/it]Device set to use cpu\n",
      "Run through video frame per frame:   4%|▍         | 23/554 [01:30<39:01,  4.41s/it]Device set to use cpu\n",
      "Run through video frame per frame:   4%|▍         | 24/554 [01:34<37:16,  4.22s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▍         | 25/554 [01:38<36:05,  4.09s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▍         | 26/554 [01:41<34:36,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▍         | 27/554 [01:45<33:18,  3.79s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▌         | 28/554 [01:48<32:49,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▌         | 29/554 [01:53<33:33,  3.84s/it]Device set to use cpu\n",
      "Run through video frame per frame:   5%|▌         | 30/554 [01:56<32:42,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▌         | 31/554 [02:00<32:57,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▌         | 32/554 [02:04<32:56,  3.79s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▌         | 33/554 [02:08<33:44,  3.89s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▌         | 34/554 [02:13<36:27,  4.21s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▋         | 35/554 [02:16<34:34,  4.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:   6%|▋         | 36/554 [02:20<34:00,  3.94s/it]Device set to use cpu\n",
      "Run through video frame per frame:   7%|▋         | 37/554 [02:24<34:45,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:   7%|▋         | 38/554 [02:29<35:12,  4.09s/it]Device set to use cpu\n",
      "Run through video frame per frame:   7%|▋         | 39/554 [02:33<35:12,  4.10s/it]Device set to use cpu\n",
      "Run through video frame per frame:   7%|▋         | 40/554 [02:36<33:50,  3.95s/it]Device set to use cpu\n",
      "Run through video frame per frame:   7%|▋         | 41/554 [02:40<32:51,  3.84s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 42/554 [02:44<32:14,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 43/554 [02:47<31:33,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 44/554 [02:51<31:35,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 45/554 [02:54<31:09,  3.67s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 46/554 [02:58<31:13,  3.69s/it]Device set to use cpu\n",
      "Run through video frame per frame:   8%|▊         | 47/554 [03:02<30:53,  3.66s/it]Device set to use cpu\n",
      "Run through video frame per frame:   9%|▊         | 48/554 [03:05<30:49,  3.66s/it]Device set to use cpu\n",
      "Run through video frame per frame:   9%|▉         | 49/554 [03:10<33:13,  3.95s/it]Device set to use cpu\n",
      "Run through video frame per frame:   9%|▉         | 50/554 [03:14<33:02,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:   9%|▉         | 51/554 [03:18<33:43,  4.02s/it]Device set to use cpu\n",
      "Run through video frame per frame:   9%|▉         | 52/554 [03:22<34:18,  4.10s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|▉         | 53/554 [03:29<41:33,  4.98s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|▉         | 54/554 [03:35<42:43,  5.13s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|▉         | 55/554 [03:39<39:52,  4.80s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|█         | 56/554 [03:43<38:49,  4.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|█         | 57/554 [03:48<38:43,  4.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:  10%|█         | 58/554 [03:53<39:10,  4.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:  11%|█         | 59/554 [03:57<37:38,  4.56s/it]Device set to use cpu\n",
      "Run through video frame per frame:  11%|█         | 60/554 [04:01<37:04,  4.50s/it]Device set to use cpu\n",
      "Run through video frame per frame:  11%|█         | 61/554 [04:05<34:28,  4.20s/it]Device set to use cpu\n",
      "Run through video frame per frame:  11%|█         | 62/554 [04:09<33:13,  4.05s/it]Device set to use cpu\n",
      "Run through video frame per frame:  11%|█▏        | 63/554 [04:12<32:35,  3.98s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 64/554 [04:16<31:25,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 65/554 [04:20<33:00,  4.05s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 66/554 [04:24<32:13,  3.96s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 67/554 [04:28<31:25,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 68/554 [04:32<31:09,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  12%|█▏        | 69/554 [04:35<30:32,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  13%|█▎        | 70/554 [04:39<30:22,  3.77s/it]Device set to use cpu\n",
      "Run through video frame per frame:  13%|█▎        | 71/554 [04:43<31:14,  3.88s/it]Device set to use cpu\n",
      "Run through video frame per frame:  13%|█▎        | 72/554 [04:47<31:16,  3.89s/it]Device set to use cpu\n",
      "Run through video frame per frame:  13%|█▎        | 73/554 [04:51<31:39,  3.95s/it]Device set to use cpu\n",
      "Run through video frame per frame:  13%|█▎        | 74/554 [04:55<31:12,  3.90s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▎        | 75/554 [05:00<32:55,  4.12s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▎        | 76/554 [05:04<32:30,  4.08s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▍        | 77/554 [05:07<31:37,  3.98s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▍        | 78/554 [05:11<30:24,  3.83s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▍        | 79/554 [05:15<30:46,  3.89s/it]Device set to use cpu\n",
      "Run through video frame per frame:  14%|█▍        | 80/554 [05:19<30:49,  3.90s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▍        | 81/554 [05:22<30:22,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▍        | 82/554 [05:27<30:57,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▍        | 83/554 [05:31<31:38,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▌        | 84/554 [05:35<31:18,  4.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:  15%|█▌        | 85/554 [05:39<30:52,  3.95s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▌        | 86/554 [05:43<30:48,  3.95s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▌        | 87/554 [05:47<31:41,  4.07s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▌        | 88/554 [05:51<30:46,  3.96s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▌        | 89/554 [05:54<29:58,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▌        | 90/554 [05:58<29:42,  3.84s/it]Device set to use cpu\n",
      "Run through video frame per frame:  16%|█▋        | 91/554 [06:02<28:59,  3.76s/it]Device set to use cpu\n",
      "Run through video frame per frame:  17%|█▋        | 92/554 [06:05<28:58,  3.76s/it]Device set to use cpu\n",
      "Run through video frame per frame:  17%|█▋        | 93/554 [06:09<28:36,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  17%|█▋        | 94/554 [06:13<28:04,  3.66s/it]Device set to use cpu\n",
      "Run through video frame per frame:  17%|█▋        | 95/554 [06:16<28:18,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  17%|█▋        | 96/554 [06:20<28:16,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 97/554 [06:24<27:56,  3.67s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 98/554 [06:27<27:29,  3.62s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 99/554 [06:31<28:03,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 100/554 [06:35<27:49,  3.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 101/554 [06:39<29:03,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  18%|█▊        | 102/554 [06:45<33:29,  4.45s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▊        | 103/554 [06:50<35:33,  4.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▉        | 104/554 [06:54<32:57,  4.39s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▉        | 105/554 [06:57<31:11,  4.17s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▉        | 106/554 [07:02<31:12,  4.18s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▉        | 107/554 [07:07<32:55,  4.42s/it]Device set to use cpu\n",
      "Run through video frame per frame:  19%|█▉        | 108/554 [07:10<31:36,  4.25s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|█▉        | 109/554 [07:14<30:09,  4.07s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|█▉        | 110/554 [07:18<29:46,  4.02s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|██        | 111/554 [07:22<28:34,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|██        | 112/554 [07:25<28:32,  3.88s/it]Device set to use cpu\n",
      "Run through video frame per frame:  20%|██        | 113/554 [07:30<29:11,  3.97s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██        | 114/554 [07:34<29:31,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██        | 115/554 [07:38<28:51,  3.94s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██        | 116/554 [07:41<28:35,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██        | 117/554 [07:45<28:04,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██▏       | 118/554 [07:49<28:08,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  21%|██▏       | 119/554 [07:54<29:45,  4.10s/it]Device set to use cpu\n",
      "Run through video frame per frame:  22%|██▏       | 120/554 [07:58<31:20,  4.33s/it]Device set to use cpu\n",
      "Run through video frame per frame:  22%|██▏       | 121/554 [08:04<32:49,  4.55s/it]Device set to use cpu\n",
      "Run through video frame per frame:  22%|██▏       | 122/554 [08:08<31:53,  4.43s/it]Device set to use cpu\n",
      "Run through video frame per frame:  22%|██▏       | 123/554 [08:13<32:37,  4.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  22%|██▏       | 124/554 [08:16<30:24,  4.24s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 125/554 [08:20<29:05,  4.07s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 126/554 [08:24<28:53,  4.05s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 127/554 [08:28<29:59,  4.21s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 128/554 [08:33<29:55,  4.21s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 129/554 [08:37<29:39,  4.19s/it]Device set to use cpu\n",
      "Run through video frame per frame:  23%|██▎       | 130/554 [08:41<29:25,  4.16s/it]Device set to use cpu\n",
      "Run through video frame per frame:  24%|██▎       | 131/554 [08:45<30:09,  4.28s/it]Device set to use cpu\n",
      "Run through video frame per frame:  24%|██▍       | 132/554 [08:49<29:52,  4.25s/it]Device set to use cpu\n",
      "Run through video frame per frame:  24%|██▍       | 133/554 [08:54<29:59,  4.27s/it]Device set to use cpu\n",
      "Run through video frame per frame:  24%|██▍       | 134/554 [08:58<29:04,  4.15s/it]Device set to use cpu\n",
      "Run through video frame per frame:  24%|██▍       | 135/554 [09:03<31:05,  4.45s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▍       | 136/554 [09:08<33:15,  4.77s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▍       | 137/554 [09:13<33:12,  4.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▍       | 138/554 [09:17<32:03,  4.62s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▌       | 139/554 [09:22<30:59,  4.48s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▌       | 140/554 [09:27<32:12,  4.67s/it]Device set to use cpu\n",
      "Run through video frame per frame:  25%|██▌       | 141/554 [09:33<35:31,  5.16s/it]Device set to use cpu\n",
      "Run through video frame per frame:  26%|██▌       | 142/554 [09:38<35:28,  5.17s/it]Device set to use cpu\n",
      "Run through video frame per frame:  26%|██▌       | 143/554 [09:43<33:43,  4.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  26%|██▌       | 144/554 [09:47<33:17,  4.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  26%|██▌       | 145/554 [09:53<34:04,  5.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:  26%|██▋       | 146/554 [09:58<34:54,  5.13s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 147/554 [10:02<32:40,  4.82s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 148/554 [10:06<30:18,  4.48s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 149/554 [10:09<28:16,  4.19s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 150/554 [10:13<26:56,  4.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 151/554 [10:17<26:34,  3.96s/it]Device set to use cpu\n",
      "Run through video frame per frame:  27%|██▋       | 152/554 [10:20<25:54,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  28%|██▊       | 153/554 [10:25<27:19,  4.09s/it]Device set to use cpu\n",
      "Run through video frame per frame:  28%|██▊       | 154/554 [10:29<26:50,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  28%|██▊       | 155/554 [10:33<26:56,  4.05s/it]Device set to use cpu\n",
      "Run through video frame per frame:  28%|██▊       | 156/554 [10:37<26:18,  3.96s/it]Device set to use cpu\n",
      "Run through video frame per frame:  28%|██▊       | 157/554 [10:41<26:01,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▊       | 158/554 [10:44<25:33,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▊       | 159/554 [10:48<25:48,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▉       | 160/554 [10:52<25:44,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▉       | 161/554 [10:57<27:59,  4.27s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▉       | 162/554 [11:01<27:34,  4.22s/it]Device set to use cpu\n",
      "Run through video frame per frame:  29%|██▉       | 163/554 [11:05<26:16,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|██▉       | 164/554 [11:09<26:13,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|██▉       | 165/554 [11:13<25:40,  3.96s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|██▉       | 166/554 [11:17<24:57,  3.86s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|███       | 167/554 [11:20<24:26,  3.79s/it]Device set to use cpu\n",
      "Run through video frame per frame:  30%|███       | 168/554 [11:24<24:20,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███       | 169/554 [11:28<24:57,  3.89s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███       | 170/554 [11:32<25:50,  4.04s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███       | 171/554 [11:37<25:56,  4.06s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███       | 172/554 [11:42<27:45,  4.36s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███       | 173/554 [11:46<27:01,  4.26s/it]Device set to use cpu\n",
      "Run through video frame per frame:  31%|███▏      | 174/554 [11:50<27:18,  4.31s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 175/554 [11:55<27:48,  4.40s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 176/554 [11:59<26:47,  4.25s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 177/554 [12:02<25:32,  4.07s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 178/554 [12:06<25:04,  4.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 179/554 [12:10<24:32,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:  32%|███▏      | 180/554 [12:14<24:18,  3.90s/it]Device set to use cpu\n",
      "Run through video frame per frame:  33%|███▎      | 181/554 [12:18<24:11,  3.89s/it]Device set to use cpu\n",
      "Run through video frame per frame:  33%|███▎      | 182/554 [12:21<23:40,  3.82s/it]Device set to use cpu\n",
      "Run through video frame per frame:  33%|███▎      | 183/554 [12:25<23:07,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:  33%|███▎      | 184/554 [12:28<22:48,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  33%|███▎      | 185/554 [12:32<22:52,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▎      | 186/554 [12:36<22:44,  3.71s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▍      | 187/554 [12:39<22:39,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▍      | 188/554 [12:43<22:44,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▍      | 189/554 [12:47<22:30,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▍      | 190/554 [12:51<22:33,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  34%|███▍      | 191/554 [12:54<22:15,  3.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▍      | 192/554 [12:58<22:10,  3.68s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▍      | 193/554 [13:02<22:43,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▌      | 194/554 [13:06<22:21,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▌      | 195/554 [13:09<22:26,  3.75s/it]Device set to use cpu\n",
      "Run through video frame per frame:  35%|███▌      | 196/554 [13:13<23:01,  3.86s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▌      | 197/554 [13:18<24:23,  4.10s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▌      | 198/554 [13:23<25:01,  4.22s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▌      | 199/554 [13:26<23:55,  4.04s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▌      | 200/554 [13:30<23:32,  3.99s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▋      | 201/554 [13:34<23:06,  3.93s/it]Device set to use cpu\n",
      "Run through video frame per frame:  36%|███▋      | 202/554 [13:38<22:33,  3.85s/it]Device set to use cpu\n",
      "Run through video frame per frame:  37%|███▋      | 203/554 [13:41<22:16,  3.81s/it]Device set to use cpu\n",
      "Run through video frame per frame:  37%|███▋      | 204/554 [13:45<22:01,  3.77s/it]Device set to use cpu\n",
      "Run through video frame per frame:  37%|███▋      | 205/554 [13:49<21:33,  3.71s/it]Device set to use cpu\n",
      "Run through video frame per frame:  37%|███▋      | 206/554 [13:52<21:38,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  37%|███▋      | 207/554 [13:56<21:40,  3.75s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 208/554 [14:00<21:27,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 209/554 [14:04<21:51,  3.80s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 210/554 [14:08<22:25,  3.91s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 211/554 [14:12<22:24,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 212/554 [14:16<22:13,  3.90s/it]Device set to use cpu\n",
      "Run through video frame per frame:  38%|███▊      | 213/554 [14:20<22:25,  3.94s/it]Device set to use cpu\n",
      "Run through video frame per frame:  39%|███▊      | 214/554 [14:24<23:02,  4.06s/it]Device set to use cpu\n",
      "Run through video frame per frame:  39%|███▉      | 215/554 [14:30<25:39,  4.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  39%|███▉      | 216/554 [14:34<25:16,  4.49s/it]Device set to use cpu\n",
      "Run through video frame per frame:  39%|███▉      | 217/554 [14:38<24:56,  4.44s/it]Device set to use cpu\n",
      "Run through video frame per frame:  39%|███▉      | 218/554 [14:43<24:38,  4.40s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|███▉      | 219/554 [14:47<25:07,  4.50s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|███▉      | 220/554 [14:52<24:22,  4.38s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|███▉      | 221/554 [14:56<24:15,  4.37s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|████      | 222/554 [15:01<25:27,  4.60s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|████      | 223/554 [15:05<24:54,  4.52s/it]Device set to use cpu\n",
      "Run through video frame per frame:  40%|████      | 224/554 [15:10<24:32,  4.46s/it]Device set to use cpu\n",
      "Run through video frame per frame:  41%|████      | 225/554 [15:13<23:20,  4.26s/it]Device set to use cpu\n",
      "Run through video frame per frame:  41%|████      | 226/554 [15:20<27:29,  5.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  41%|████      | 227/554 [15:25<26:29,  4.86s/it]Device set to use cpu\n",
      "Run through video frame per frame:  41%|████      | 228/554 [15:29<24:42,  4.55s/it]Device set to use cpu\n",
      "Run through video frame per frame:  41%|████▏     | 229/554 [15:33<23:35,  4.36s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 230/554 [15:36<22:38,  4.19s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 231/554 [15:40<21:40,  4.03s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 232/554 [15:44<20:55,  3.90s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 233/554 [15:47<20:23,  3.81s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 234/554 [15:51<19:46,  3.71s/it]Device set to use cpu\n",
      "Run through video frame per frame:  42%|████▏     | 235/554 [15:54<19:37,  3.69s/it]Device set to use cpu\n",
      "Run through video frame per frame:  43%|████▎     | 236/554 [15:58<19:15,  3.63s/it]Device set to use cpu\n",
      "Run through video frame per frame:  43%|████▎     | 237/554 [16:01<19:04,  3.61s/it]Device set to use cpu\n",
      "Run through video frame per frame:  43%|████▎     | 238/554 [16:05<18:56,  3.60s/it]Device set to use cpu\n",
      "Run through video frame per frame:  43%|████▎     | 239/554 [16:08<18:51,  3.59s/it]Device set to use cpu\n",
      "Run through video frame per frame:  43%|████▎     | 240/554 [16:13<19:35,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▎     | 241/554 [16:17<20:47,  3.99s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▎     | 242/554 [16:21<20:24,  3.92s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▍     | 243/554 [16:25<20:45,  4.00s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▍     | 244/554 [16:29<20:12,  3.91s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▍     | 245/554 [16:32<19:29,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  44%|████▍     | 246/554 [16:36<19:18,  3.76s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▍     | 247/554 [16:40<19:34,  3.82s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▍     | 248/554 [16:44<19:05,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▍     | 249/554 [16:47<19:20,  3.80s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▌     | 250/554 [16:51<19:09,  3.78s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▌     | 251/554 [16:55<18:40,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  45%|████▌     | 252/554 [16:58<18:33,  3.69s/it]Device set to use cpu\n",
      "Run through video frame per frame:  46%|████▌     | 253/554 [17:02<18:38,  3.72s/it]Device set to use cpu\n",
      "Run through video frame per frame:  46%|████▌     | 254/554 [17:06<18:40,  3.73s/it]Device set to use cpu\n",
      "Run through video frame per frame:  46%|████▌     | 255/554 [17:11<20:47,  4.17s/it]Device set to use cpu\n",
      "Run through video frame per frame:  46%|████▌     | 256/554 [17:16<21:03,  4.24s/it]Device set to use cpu\n",
      "Run through video frame per frame:  46%|████▋     | 257/554 [17:19<20:01,  4.05s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 258/554 [17:23<19:04,  3.87s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 259/554 [17:26<18:25,  3.75s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 260/554 [17:30<18:08,  3.70s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 261/554 [17:33<18:06,  3.71s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 262/554 [17:37<17:40,  3.63s/it]Device set to use cpu\n",
      "Run through video frame per frame:  47%|████▋     | 263/554 [17:40<17:34,  3.62s/it]Device set to use cpu\n",
      "Run through video frame per frame:  48%|████▊     | 264/554 [17:44<17:16,  3.57s/it]Device set to use cpu\n",
      "Run through video frame per frame:  48%|████▊     | 265/554 [17:48<18:34,  3.86s/it]Device set to use cpu\n",
      "Run through video frame per frame:  48%|████▊     | 266/554 [17:53<19:07,  3.98s/it]Device set to use cpu\n",
      "Run through video frame per frame:  48%|████▊     | 267/554 [17:56<18:17,  3.82s/it]Device set to use cpu\n",
      "Run through video frame per frame:  48%|████▊     | 268/554 [18:00<17:49,  3.74s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▊     | 269/554 [18:03<17:23,  3.66s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▊     | 270/554 [18:07<17:07,  3.62s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▉     | 271/554 [18:10<17:02,  3.61s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▉     | 272/554 [18:14<16:44,  3.56s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▉     | 273/554 [18:17<16:31,  3.53s/it]Device set to use cpu\n",
      "Run through video frame per frame:  49%|████▉     | 274/554 [18:21<16:30,  3.54s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|████▉     | 275/554 [18:25<16:49,  3.62s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|████▉     | 276/554 [18:28<16:50,  3.64s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|█████     | 277/554 [18:32<17:07,  3.71s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|█████     | 278/554 [18:36<17:32,  3.81s/it]Device set to use cpu\n",
      "Run through video frame per frame:  50%|█████     | 279/554 [18:40<18:13,  3.98s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████     | 280/554 [18:45<18:55,  4.14s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████     | 281/554 [18:49<19:08,  4.21s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████     | 282/554 [18:55<20:55,  4.61s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████     | 283/554 [18:59<20:11,  4.47s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████▏    | 284/554 [19:03<19:10,  4.26s/it]Device set to use cpu\n",
      "Run through video frame per frame:  51%|█████▏    | 285/554 [19:07<18:57,  4.23s/it]Device set to use cpu\n",
      "Run through video frame per frame:  52%|█████▏    | 286/554 [19:12<19:41,  4.41s/it]Device set to use cpu\n",
      "Run through video frame per frame:  52%|█████▏    | 287/554 [19:16<19:49,  4.45s/it]Device set to use cpu\n",
      "Run through video frame per frame:  52%|█████▏    | 288/554 [19:21<19:53,  4.49s/it]Device set to use cpu\n",
      "Run through video frame per frame:  52%|█████▏    | 289/554 [19:25<19:19,  4.37s/it]Device set to use cpu\n",
      "Run through video frame per frame:  52%|█████▏    | 289/554 [19:27<17:50,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m path_output = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlehrm\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDaten\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mArbeit_u_Studium\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mStudium\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m5_Master_lokal\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mrepos\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2025_p03_policy_learning\u001b[39m\u001b[33m\\\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgrayscale1.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mconvert_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprediction_models\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEPTH_ANYTHING_V2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mconvert_video\u001b[39m\u001b[34m(src_path, target_path, selected_model)\u001b[39m\n\u001b[32m     75\u001b[39m         tqdm.write(\u001b[33m\"\u001b[39m\u001b[33mWARNUNG: Kein weiteres Bild gelesen - Video zu Ende oder Fehler beim Zugriff.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     new_image = \u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     image_to_video(new_image,video_writer)\n\u001b[32m     80\u001b[39m cap.release()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mapply_model\u001b[39m\u001b[34m(frame, model_selection)\u001b[39m\n\u001b[32m     36\u001b[39m     checkpoint = \u001b[33m\"\u001b[39m\u001b[33mdepth-anything/Depth-Anything-V2-base-hf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m     pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mdepth-estimation\u001b[39m\u001b[33m\"\u001b[39m, model=checkpoint, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     predictions = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     image_w_pred = predictions[\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_selection == prediction_models.GRAYSCALE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\pipelines\\depth_estimation.py:93\u001b[39m, in \u001b[36mDepthEstimationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot call the depth-estimation pipeline without an inputs argument!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\pipelines\\depth_estimation.py:113\u001b[39m, in \u001b[36mDepthEstimationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[32m    112\u001b[39m     target_size = model_inputs.pop(\u001b[33m\"\u001b[39m\u001b[33mtarget_size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     model_outputs[\u001b[33m\"\u001b[39m\u001b[33mtarget_size\u001b[39m\u001b[33m\"\u001b[39m] = target_size\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\depth_anything\\modeling_depth_anything.py:440\u001b[39m, in \u001b[36mDepthAnythingForDepthEstimation.forward\u001b[39m\u001b[34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    435\u001b[39m output_hidden_states = (\n\u001b[32m    436\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    437\u001b[39m )\n\u001b[32m    438\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_with_filtered_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m hidden_states = outputs.feature_maps\n\u001b[32m    445\u001b[39m _, _, height, width = pixel_values.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\utils\\backbone_utils.py:235\u001b[39m, in \u001b[36mBackboneMixin.forward_with_filtered_kwargs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m signature = \u001b[38;5;28mdict\u001b[39m(inspect.signature(\u001b[38;5;28mself\u001b[39m.forward).parameters)\n\u001b[32m    234\u001b[39m filtered_kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature}\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\dinov2\\modeling_dinov2.py:869\u001b[39m, in \u001b[36mDinov2Backbone.forward\u001b[39m\u001b[34m(self, pixel_values, output_hidden_states, output_attentions, return_dict)\u001b[39m\n\u001b[32m    865\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m    867\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values)\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    873\u001b[39m hidden_states = outputs.hidden_states \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    875\u001b[39m feature_maps = ()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\dinov2\\modeling_dinov2.py:492\u001b[39m, in \u001b[36mDinov2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    485\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    486\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    487\u001b[39m         hidden_states,\n\u001b[32m    488\u001b[39m         layer_head_mask,\n\u001b[32m    489\u001b[39m         output_attentions,\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\dinov2\\modeling_dinov2.py:433\u001b[39m, in \u001b[36mDinov2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    429\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    430\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    431\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    432\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in Dinov2, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    440\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.layer_scale1(attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\dinov2\\modeling_dinov2.py:319\u001b[39m, in \u001b[36mDinov2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    314\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    315\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    316\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    317\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    318\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    323\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\transformers\\models\\dinov2\\modeling_dinov2.py:235\u001b[39m, in \u001b[36mDinov2SelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m, hidden_states, head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    234\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    236\u001b[39m     value_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.value(hidden_states))\n\u001b[32m    237\u001b[39m     query_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.query(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "path_output = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\output\\grayscale1.mp4\"\n",
    "convert_video(path_example, path_output,prediction_models.DEPTH_ANYTHING_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db7f07",
   "metadata": {},
   "source": [
    "### Test Datei-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba0141e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000000.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000001.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000002.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000003.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000004.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000005.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000006.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000007.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000008.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000009.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000010.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000011.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000012.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000013.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000014.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000015.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000016.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000017.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000018.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000019.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000020.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000021.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000022.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000023.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000024.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000025.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000026.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000027.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000028.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000029.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000030.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000031.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000032.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000033.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000034.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000035.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000036.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000037.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000038.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000039.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000040.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000041.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000042.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000043.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000044.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000045.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000046.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000047.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000048.mp4\n",
      "C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\\episode_000049.mp4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\lehrm\\Daten\\Arbeit_u_Studium\\Studium\\5_Master_lokal\\repos\\2025_p03_policy_learning\\dataset\\studytable_open_drawer\\videos\\chunk-000\\observation.image.camera1_img\"\n",
    "\n",
    "mp4_files = glob.glob(os.path.join(path,\"*.mp4\"))\n",
    "\n",
    "for datei in mp4_files:\n",
    "    print(datei)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
